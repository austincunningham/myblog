(window["webpackJsonp"]=window["webpackJsonp"]||[]).push([["chunk-2d22bce6"],{f116:function(s,a,e){"use strict";e.r(a);var t=function(){var s=this,a=s.$createElement;s._self._c;return s._m(0)},r=[function(){var s=this,a=s.$createElement,e=s._self._c||a;return e("section",[e("h1",[s._v("Build a Local AI RAG App with Ollama and Python")]),e("p",[e("img",{attrs:{src:"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/fkwt1jqnwkfrax0iqlvn.png",alt:""}})]),e("p",[s._v("I was looking to do a little development around AI and decided to see what I could build on my PC.")]),e("p",[s._v("We tried "),e("a",{attrs:{href:"https://ollama.com/download/linux"}},[s._v("Ollama")]),s._v(" for a local development environment. Ollama is an open-source tool that makes it easy to download, run, and manage large language models (LLMs) on your own computer.")]),e("pre",{pre:!0},[e("code",{pre:!0,attrs:{"v-pre":"",class:"language-bash"}},[s._v("curl -fsSL https://ollama.com/install.sh | sh\n")])]),e("p",[s._v("Once downloaded and installed you can run as follows")]),e("pre",{pre:!0},[e("code",{pre:!0,attrs:{"v-pre":"",class:"language-bash"}},[s._v("~ ollama -v         \nollama version is 0.6.5\n~ ollama run mistral\n>>> tell me about ollama run mistral\n "),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v('"Olama Run Mistral is a high-performance, open-source machine learning platform developed by OLMA.AI, a company based in Paris, France. The platform is designed to simplify the development and deployment of large-scale machine \nlearning models with a focus on deep learning.\nMistral stands out for its scalability, flexibility, and ease of use. It\'s built around a distributed architecture that allows users to train and deploy large-scale machine learning models efficiently across multiple GPUs and \nclusters, reducing the time required for training and inference significantly.\nSome key features of Olama"')]),s._v(" "),e("span",{pre:!0,attrs:{class:"hljs-comment"}},[s._v("# ====> you get the idea a chatbot in your terminal")]),s._v("\n")])]),e("p",[s._v("There is a bunch of models you can run https://ollama.com/search we will stick to mistral for the time being. And we need to keep it running to use it.")]),e("p",[s._v("So what are we going to try to do ? We are going to create an chat app that can query my local files directory and return answers based off the contents.")]),e("p",[s._v("First create requirements.txt and add the following, basically some libraries for interacting with text and ollama")]),e("pre",{pre:!0},[e("code",{pre:!0,attrs:{"v-pre":"",class:"language-bash"}},[s._v("langchain\nlangchain-community\nlangchain-ollama\nlangchain-huggingface\nchromadb\nsentence-transformers\n")])]),e("p",[s._v("Setup a virtual environment and install the requirements")]),e("pre",{pre:!0},[e("code",{pre:!0,attrs:{"v-pre":"",class:"language-bash"}},[s._v("sudo dnf install gcc-c++ python3-devel\npip install langchain chromadb sentence-transformers ollama\npython3 -m venv venv\n"),e("span",{pre:!0,attrs:{class:"hljs-built_in"}},[s._v("source")]),s._v(" venv/bin/activate\npip install -r requirements.txt \n")])]),e("p",[s._v("I create two files a main.py and utils/loaders.py")]),e("h2",[s._v("utils/loaders.py")]),e("pre",{pre:!0},[e("code",{pre:!0,attrs:{"v-pre":"",class:"language-python"}},[e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("from")]),s._v(" langchain_community.document_loaders "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("import")]),s._v(" DirectoryLoader, TextLoader\n\n"),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("import")]),s._v(" os\n\n\n"),e("span",{pre:!0,attrs:{class:"hljs-function"}},[e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("def")]),s._v(" "),e("span",{pre:!0,attrs:{class:"hljs-title"}},[s._v("load_sop_files")]),e("span",{pre:!0,attrs:{class:"hljs-params"}},[s._v("(directory: str)")]),s._v(":")]),s._v("\n    allowed_exts = ("),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v("'.md'")]),s._v(", "),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v("'.asciidoc'")]),s._v(", "),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v("'.txt'")]),s._v(")\n\n    docs = []\n    "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("for")]),s._v(" root, _, files "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("in")]),s._v(" os.walk(directory):\n        "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("for")]),s._v(" file "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("in")]),s._v(" files:\n            "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("if")]),s._v(" file.lower().endswith(allowed_exts):\n                path = os.path.join(root, file)\n                "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("try")]),s._v(":\n                    loader = TextLoader(path, encoding="),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v("'utf-8'")]),s._v(")\n                    docs.extend(loader.load())\n                "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("except")]),s._v(" Exception "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("as")]),s._v(" e:\n                    print("),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v('f"‚ùå Error loading '),e("span",{pre:!0,attrs:{class:"hljs-subst"}},[s._v("{path}")]),s._v(": "),e("span",{pre:!0,attrs:{class:"hljs-subst"}},[s._v("{e}")]),s._v('"')]),s._v(")\n    "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("return")]),s._v(" docs\n")])]),e("p",[s._v("utils/loaders.py function takes in a directory path and goes through all the files and finds type md,asciidoc and txt and breaks them up into an array utf-8 formatted langchain docs.")]),e("h2",[s._v("main.py")]),e("p",[s._v("First thing I do in main.py is add the libraries and then call the function in utils/loaders.py to load my docs into the application.")]),e("pre",{pre:!0},[e("code",{pre:!0,attrs:{"v-pre":"",class:"language-python"}},[e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("from")]),s._v(" utils.loaders "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("import")]),s._v(" load_sop_files \n"),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("from")]),s._v(" langchain.text_splitter "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("import")]),s._v(" \n"),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("from")]),s._v(" langchain_community.embeddings "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("import")]),s._v(" HuggingFaceEmbeddings\n"),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("from")]),s._v(" langchain_community.vectorstores "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("import")]),s._v(" Chroma\n"),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("from")]),s._v(" langchain.chains "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("import")]),s._v(" RetrievalQA\n"),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("from")]),s._v(" langchain_ollama "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("import")]),s._v(" OllamaLLM\n"),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("from")]),s._v(" langchain_huggingface "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("import")]),s._v(" HuggingFaceEmbeddings\n\n\n\n\n"),e("span",{pre:!0,attrs:{class:"hljs-comment"}},[s._v("# Load and prepare documents")]),s._v("\nprint("),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v('"üìÇ Loading SOP documents..."')]),s._v(")\n"),e("span",{pre:!0,attrs:{class:"hljs-comment"}},[s._v("# pointing to local directory that is the same level as this project")]),s._v("\ndocs = load_sop_files("),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v('"../help/sops/"')]),s._v(")\n")])]),e("p",[s._v("LLM's need data split up into smaller sizes")]),e("pre",{pre:!0},[e("code",{pre:!0,attrs:{"v-pre":"",class:"language-python"}},[s._v("splitter = RecursiveCharacterTextSplitter(chunk_size="),e("span",{pre:!0,attrs:{class:"hljs-number"}},[s._v("500")]),s._v(", chunk_overlap="),e("span",{pre:!0,attrs:{class:"hljs-number"}},[s._v("100")]),s._v(")\nchunks = splitter.split_documents(docs)\n")])]),e("p",[s._v("We then need to convert the document data into a numeric format that can be handled by the LLM")]),e("pre",{pre:!0},[e("code",{pre:!0,attrs:{"v-pre":"",class:"language-python"}},[s._v("\nprint("),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v('"üß† Creating vector database..."')]),s._v(")\nembeddings = HuggingFaceEmbeddings(model_name="),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v('"all-MiniLM-L6-v2"')]),s._v(")\ndb = Chroma.from_documents(chunks, embeddings)\n")])]),e("p",[s._v("We set up Ollama mistral to take the numeric db data and use to\nto formulate its answers, We are not training a modal here but\nusing the RAG(Retrieval-Augmented Generation) pattern")]),e("pre",{pre:!0},[e("code",{pre:!0,attrs:{"v-pre":"",class:"language-python"}},[s._v("retriever = db.as_retriever()\nllm = OllamaLLM(model="),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v('"mistral"')]),s._v(")\nqa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents="),e("span",{pre:!0,attrs:{class:"hljs-literal"}},[s._v("True")]),s._v(")\n")])]),e("blockquote",[e("p",[e("strong",[s._v("NOTE :")]),s._v(" We are using the OllamaLLM and RetrievalQA to connect to our local LLM API but we could use the "),e("a",{attrs:{href:"https://github.com/ollama/ollama/blob/main/docs/api.md"}},[s._v("Ollama API")]),s._v(" here with the "),e("code",{pre:!0},[s._v("/api/generate")]),s._v(" end point, but this would be more verbose.")])]),e("p",[s._v("We then have a loop here to answer queries using the qa we created earlier as a source with the invoke function")]),e("pre",{pre:!0},[e("code",{pre:!0,attrs:{"v-pre":"",class:"language-python"}},[s._v("\nprint("),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v("\"ü§ñ SOP Assistant ready. Type your question below. Type 'exit' to quit.\"")]),s._v(")\n\n\n\n"),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("while")]),s._v(" "),e("span",{pre:!0,attrs:{class:"hljs-literal"}},[s._v("True")]),s._v(":\n   query = input("),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v('"\\nüìù You: "')]),s._v(")\n   "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("if")]),s._v(" query.lower() "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("in")]),s._v(" ("),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v('"exit"')]),s._v(", "),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v('"quit"')]),s._v("):\n       print("),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v('"üëã Bye! Take care."')]),s._v(")\n       "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("break")]),s._v("\n\n\n   result = qa.invoke({"),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v('"query"')]),s._v(": query})\n\n\n   print("),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v('"\\nü§ñ Assistant:\\n"')]),s._v(", result["),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v('"result"')]),s._v("])\n\n\n   print("),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v('"\\nüìé Sources:"')]),s._v(")\n   "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("for")]),s._v(" doc "),e("span",{pre:!0,attrs:{class:"hljs-keyword"}},[s._v("in")]),s._v(" result["),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v('"source_documents"')]),s._v("]:\n       print("),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v('f" - '),e("span",{pre:!0,attrs:{class:"hljs-subst"}},[s._v("{doc.metadata.get("),e("span",{pre:!0,attrs:{class:"hljs-string"}},[s._v("'source'")]),s._v(")}")]),s._v('"')]),s._v(")\n")])]),e("p",[s._v("Repo https://github.com/austincunningham/sop_assistant")]),e("p",[s._v("It's returning answers based off the "),e("code",{pre:!0},[s._v("../help/sops")]),s._v(" directory we passed in earlier")]),e("p",[e("img",{attrs:{src:"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/l21ciivffm9y8h2fjwvw.png",alt:"Image description"}})]),e("p",[s._v("Credit to https://github.com/valerymo for a lot of the investigation on getting this operational.")])])}],n=e("2877"),l={},o=Object(n["a"])(l,t,r,!1,null,null,null);a["default"]=o.exports}}]);
//# sourceMappingURL=chunk-2d22bce6.f198f201.js.map